properties:
  workloadProfileName: gpu-a100
  configuration:
    activeRevisionsMode: Single
    ingress:
      external: true
      targetPort: 8000
      transport: Auto
  template:
    containers:
    - name: vllm-inference
      image: vllm/vllm-openai:latest
      resources:
        cpu: 12.0
        memory: 32Gi
      command:
      - python3
      - -m
      - vllm.entrypoints.openai.api_server
      args:
      - --model
      - deepseek-ai/DeepSeek-OCR
      - --trust-remote-code
      - --gpu-memory-utilization
      - "0.9"
      - --max-model-len
      - "8192"
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      probes:
      - type: Startup
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 60
        periodSeconds: 10
        failureThreshold: 30
    scale:
      minReplicas: 1
      maxReplicas: 1
